---
layout: post
title:  "自然语言处理综论"
categories: NLP
tags: NLP  
author: fly
---

* content
{:toc}




### 自然语言处理综论  第四章n元语法（总结）
通过(n-1)-gram 预测第N个单词的语言模型。
unigram 为通过一个单词预测下一个单词
bigram  为通过两个单词预测下一个单词
trigram  为通过三个单词预测下一个单词
这种统计概率的语言模型是基于语料库的。

#### Language Models，LM
计算下一个单词的概率 与计算单词序列的概率密切相关。倾向于选择出现概率高的词或者句子。在句子无限长的情况下，可以得到较好预测整体准确率。

机器翻译中Veterbi算法后，通过N-gram判断句子通畅度， 手写识别和语音识别的N-gram纠错等。

预测下一个单词的能力对于残疾人的增强交际系统是至关重要的

#### 4.1 语料库中单词数目的计算

概率的计算依赖于语料库(corpus), 计算单词的概率，基于Brown语料库规模为一百万单词的语料库

#### 4.2 简单的（非平滑的）N元语法

计算unigram 预测下一个词概率？通过语料库计算unigram 两个词联合分布。近似地逼近（approximate）该单词的历史。基于马尔可夫假设的unigram 预测下一个单词。
P(Wn | W(n-1,n-N-1)) ~ 逼近 N-gram 逼近全部单词序列预测下一个单词

#### 4.3 平滑的N元语法

