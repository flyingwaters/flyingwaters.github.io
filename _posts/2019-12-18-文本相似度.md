---
layout: post
title:  "文本相似度"
categories: NLP
tags:  DL  NLP  
author: fly
---

* content
{:toc}

# NLP 的文本相似度(短语、单词、句子)
## 前言
    自然语言处理中，会经常涉及几种计算两个文本相似度的问题。提升信息检索系统的召回率，问答系统中的，需要计算的query和候选集的相似度; 
## 文本相似度方法
* 基于统计的方法     主要是无监督学习，句子、段落, 较大粒度的文本。
* 基于语义的方法     大部分是基于深度学习的有监督学习，一般词语或句子, 较小粒度的文本。

## 文章结构    
1.  常见的几种相似性比较方法

- 基于词向量

- 基于字符

- 基于概率统计

- 基于词嵌入模型

2.  State of the art 的几种代表性方法

## 第一部分
1. 莱文斯坦距离(编辑距离)
一个字符串改成另一个字符串的最少编辑操作，“度假自由行”和“自由行度假”，中文意思完全一样，但是编辑距离是5， 表示相似度很低。 

2. 欧式和余弦距离
基于向量空间的距离度量方法

###  和BERT的相似度计算的对比， BERT+COSINE的度量
1. 利用Bert计算单词或者短语之间的相似度。

中国足协和中国足球的向量相似度: 0.94828343
中国足协和中国篮协的向量相似度: 0.9531492
自由行度假和度假自由行相似度: 0.97110826
问题：

明显看出自由行度假和度假自由行的相似度，是高于前两个的，证明BERT体系可以很好起到编辑距离的作用。

3.  N-gram 距离（BLEU核心思想）
Similarity  = |Gn(S)|+|Gt(T)|-2*|Gn(S)|*|Gn(T)|
字符串完全相等 == 0 

- 中国足协和中国足球的向量相似度: 0.94828343
- 中国足协和中国篮协的向量相似度: 0.9531492
- 中国足协和足协中国的向量相似度 0.9493892
- 美丽中国和中国美丽的向量相似度 0.95919204
- 自由行度假和度假自由行相似度: 0.97110826
相对于N-gram的距离，加入更多语义信息作为词的表示

4. BM25 
OKapi BM25 (BM代表最佳匹配）是搜索引擎根据其与给定的搜索查询的相关性对匹配文档进行排名的排名函数。Stenphen E. Robertson 和KarenSparckJones 等人开发的概率检索框架。
query -> 若干结果D，


5.  WMD
EMD 在NLP领域的延伸。EMD距离是一类运输问题。 
WMD是2015年提出的一种衡量文本相似度的方法，world  Mover's Distance@ (简称)WMD ,词移距离，度量两个文档之间的相似度，即WMD距离越小相似度越大。

DOC2VEC， simHash原理介绍






 # the some ideas 
 1. Features 
        three kind of features：
            1.  word embedding 
            2. Sentence embeddings (Doc2Vec,  Sent2Vec)
            3.  Encode question pair using dense layer from ESIM model trained on SNLI
 2. Classical text mining features
    - Similarity meatures  on LDA  and LSI embedding
    - Similarity meatures on bag of character n-grams (TFIDF reweighted or not )  from 1 to 8 gram
    - Abhishek's and owl's kindly shared features 
    - Edit and sequence matching distance ,  percentage of  common tokens up to 1,2,.....,6 when question ends the same,  or starts the same 

    - Length of  questions, diff of  legnth
    - Number of capital letters,  question marks etc... 
    - Indicators  for  Question 1/2 starting  with  "are"," Can", "How" etc .... and all mathematical engineering corresponding 

###  Tokenizer : stanford corenlp to tokenizer
###   Postagger and Ner :  postagger and  ner to preprocessing text input for some deep learning models

#  Structural features (i.e from graph) 
#### 使用图提取特征 
#### 如何建图和使用图提取特征～
1.  We  built density features from the graph built  from the edges  between pairs of questions inside  train and  test datasets  concatenated. 

2.  We  had  counts  of   neighbors  of  question  1,  question  2,  the min,  the max,  intersections, unions,  shortest  path  length  when main edge cut.....

3. We went further  and built density features to count the neighbors of  the  questions neighbors..... and  questions neighbors neighbors ... (inception)

4. We also counted neighbors  of higher order which also were  neighbors  of  lower  order (loops) 

5. We tried  different  graph  structures  :  we built undirected and directed  graphs  (edges  directed from question 1 to question 2 ),  we  also tried to separate the  density features  of question 1 from the features  of  question 2 to generate non commutative（交换的）features  in addition to commutative ones

6. We built features describing the connex((连通) subgraph the pair belonged to :  Number of edges ,  number of nodes, % of  edge in train 

7. We also computed the same features on subgrphs built only from the edges of questions which both appear more than once.我们想做的是移除fake questions which we thought were damaging the graph features by changing its  structure.

8.  最后，我们给graph 增加了权重， 我们使用了一种我们相似度的特征给我们的graph增加权重。

#  Models

1. We worked on two main  architectures for our NNets: Siamese and Attention Neutral Networks.

2. Siamese LSTM with pretrained GLOVE embedding

3. Decomposable attention with pretrained FastText embedding . This model achive ~0.3 on cv 

4. ESIM with pretrained FastText embedding .  this is our best pure Deep Learning NLP model ,  it achieves ~ 0.27 on CV.  However this model take too long to run , we only add it once in the first stacking layer 

5. We noticed that DL complex  works on the first stacking layer but did not do better than simple  MLP  on second  layer 

# Bert文本相似性的典型案例 利用预训练的中文模型实现基于Bert的语义匹配模型 数据集为LCQMC数据集
https://github.com/pengming617/bert_textMatching 

# LSTM句子相似度分析
https://github.com/zqhZY/semanaly

#  query 的规则匹配
####  速度快， 可控，易于实现，高效。
1. 与FAQ库中的标问和相似文问， 进行分词、提炼出大量的概念，并将上述概念组合，构成大量的句式，
    华为mate30 是cellphone概念，askmoney概念-， "现在" 是时间概念。

# 深度学习语义匹配
    DSSM～ LSTM-DSSM
    基于BERT等预训练模型
  #   Knowledge Based Question &Answer
  基于知识体系的问答系统
  
  KBQA 最关键一步在于KG的搭建，对绝大部分NLP任务都有极大极大的加成。
  # 基于知识图谱的语义相似度匹配和智能搜索应用
  
