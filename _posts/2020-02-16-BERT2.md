---
layout: post
title:  "文本分类的SOTA结果paper分析"
categories: NLP
tags:  NLP
author: fenglongyu
---

* content
{:toc}

How to Fine-Tune BERT for Text Classiﬁcation?
作者：复旦大学
这是一篇关于利用BERT，finetune的文章，复旦大学的NLP组发在CCL
主要贡献：
1. 做了很多实验，全面彻底地探索BERT的潜力，提出一种learningRate的方法，最顶层2e-5 其他的学习率4e-4作者说不容易拟合，batch_size 基本选用16和32开始，没降低一层 LR*0.95，做了相应实验验证这种finetune，是有效的。主要分析是，约低的transformer层抽取的特征约通用，然后LR需要小一点，每层之间的关系，经过他们的实验，最佳设定了0.95，（这个东西我们自己也可以的调整，但是既然有这个结果我们可以先用。这个问题是一个指数问题，归根结底没法求最优，凭借经验是不可避免的，所以我认为发布这篇论文的团队，做出了很偏应用的贡献，对NLP初学或者2、3年经验的人借鉴意义很大，毕竟你不可能自己做完全部探索，这就是论文的意义，我们要和世界的NLP研究者一同前行~~，哈哈哈啊哈哈哈）

2. 实验部分介绍了一些，hyperparameters设置，（1）基于1 TITAN Xp, 基于BERT-base的further的 Pre-Train给出几个超参数设置，batch_size = 32 max squence length of 128, LR = 5e-5 train_steps 100000  and warm up 10000,（warm up是一种预热学习率，第一次由何凯明在resnet中提出，还有gradual warm up等变形，可以让随机初始化的参数更加稳定，相当于小学习率，寻求方向性的修正，防止过大的学习率直接走错方向，来回振荡，导致学习时间过长）（2）基于BERT的finetune 方法，这个方法基于4titan XP(GPU),warm proportion is 0.1. 2e-5 =lr, Adam 参数$\beta_{1}=0.9$   $\beta_{2} = 0.999$

3. 探索了BERT各个层的特征用于文本分类的效果，这个应该很早有人做过，还有人将所有层的特征集合起来，类似集成的方法，然后这个结果作分类预测。这部分启发意义不大。
   
4. 长文本的prepocessing问题，这部分充分，head截取，tail截取，head+tail等，这部分结论为，前128+tail382，这也是一个指数问题寻求最优解，所以大体凭借经验，文中一个词，显示很明显，**empirically**。（有想法的话也可以自己试试别的选择）

5. pretrained in domain or cross domain 结论为，pretrained过后一定会有一定的作用，细节没细讲，因为设计数据分布的不同，太细没啥意义。最重要结论就是pretrained一遍，然后再finetune结果极大概率是好的。。
   
6. 最后，这篇论文特别适合调参参考~~~，没太多数学基础，实用的风格。。不错哦，接地气。

## 武汉加油！2020年春节，大范围的疫情铺开，没有聚会，支持武汉一线的医生。白衣天使在拯救大家，希望未来公共卫生体系更加建全，公开，透明。