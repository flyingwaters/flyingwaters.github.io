---
layout: post
title:  "Bert总结篇"
categories: NLP
tags:  NLP
author: fenglongyu
---

* content
{:toc}

### 2019过去了，人生如白驹过隙，真的很快

 人工智能在慢慢一点点发展，BERT，基于两个思想，word2vec <---- 条件概率 <---- n-gram语言模型， 看了word2vec精彩的数学推论，然后看了看bert，突然有个想法，AI是一群孜孜不倦的人共同发展出来的，它还需要更多的人去拓展。
#### 题外话
 1. 我感觉nlp发展有时真的靠直觉，人类最淳朴的直觉。当然最基本的还是有基础知识。。。。。
 2. BERT模型，由seq2seq 或者说encoder-decoder 这没记错是hinton老爷子，搞出来的，被称为神经网络的表示学习，表示学习，也就是说信息冗余，每个数据都是冗余，我们可以用很少的知识恢复它，这不是提取本质，抛开虚伪直达本真~~~也许不准确，换种说法更加高级而且灵活的，自适应SVD.。。。。
 3. BERT乃是nlp面试必考，请大家务必搞熟，另外作为nlp预训练模型的基石，我想掌握BERT理论架构，并且熟练使用它，显然是必须的，这是一系列的笔记。关于BERT理论和应用以及代码使用，除此之外还可能会涉及一点Transformer和Attention的历史以及现状。 









### BERT的细节
如何看懂这是一个很稳的问题：
首先对于整体结构，我们可以百度、google然后打开blog有很多说明。我关注的非常实际的两个方面：1、理论层面理解bert的好处。2、官方开源代码的使用（超参的设置）
首先我们从理论角度出发，详细分析bert模型的输入部分：
![alt 文字属性](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/09/bert_emnedding.png)
                        <center>**图 1**</center>
bert模型的输入需要三个嵌入层

1. Token Embedding层
2. Segment Embedding层
3. Position Embedding层
接下去我要细致的分析它们的运行过程和作用细节:
### token embedding 
文本每个词 ---> token embedding层，其具体过程如下每个词或者字----->(768,)表示。这里768是bert一个基础版的参数，可以称为bert-base-uncased、其他的如bert-large-uncased等为1024。每个文本单位（中文为字、英文为词）的向量表示维度是不同的。
例子如下:
![alt 文字属性](https://thumbnail0.baidupcs.com/thumbnail/72537c9ecn8566c40f27a46f983e338c?fid=221327382-250528-455884105595335&time=1578934800&rt=sh&sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-ckY7XJTUyMTQYLpnlWNUj0q%2B%2B6Y%3D&expires=8h&chkv=0&chkbd=0&chkpc=&dp-logid=302995822538476589&dp-callid=0&size=c710_u400&quality=100&vuk=-&ft=video)
            <center>**图 2**</center>
如5个汉字---> (5, 768)
值得注意的是英文中我们采用wordpiece，这种方法会将英文拆分为piece，singing--->sing  ##ing,具体细则，我们可以看[Wu et al. (2016)](https://arxiv.org/pdf/1609.08144.pdf)这里有详细的讲解。

### segment embedding
这个简单的说就是区分，输入是一个句子还是两个，如果只有一个句子，那么全是0，如果有两个句子，前一个1， 后一个为0.如图1所示。

### Position Embedding
transformer中的position encode公式如下：
$$PE(pos,2i)=sin(pos/10000^{2i/dmodel})$$
$$PE(pos,2i+1)=cos(pos/10000^{2i/dmodel})$$
其中，pos是指词语在序列中的位置。可以看出，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码。上面公式中的dmodel是模型的input维度(不够时需要padding)，论文默认是512。

这个编码公式的意思就是：给定词语的位置pos，我们可以把它编码成dmodel维的向量！也就是说，位置编码的每一个维度对应正弦曲线，波长构成了从2π到10000∗2π的等比序列。上面的位置编码是绝对位置编码。但是词语的相对位置也非常重要。这就是论文为什么要使用三角函数的原因！正弦函数能够表达相对位置信息。

主要数学依据是以下两个公式：
$$sin(α+β)=sinαcosβ+cosαsinβsin(α+β)=sinαcosβ+cosαsinβ$$
$$cos(α+β)=cosαcosβ−sinαsinβcos(α+β)=cosαcosβ−sinαsinβ$$
上面的公式说明，对于词汇之间的位置偏移k，$PE(pos+k)--->sin(\beta+\alpha)$可以表示成$PE(pos)--->sin(\alpha)$和$PE(k)--->sin(\beta)$的组合形式，这就是表达相对位置的能力！

但是Bert中是训练出来的(512, 768 or N)的lookup---> 也就是学习了位置表示。
### 合成表示
 我们已经介绍了长度为n的输入序列将获得的三种不同的向量表示，分别是：

- Token Embeddings， (1, n, 768) ，词的向量表示
- Segment Embeddings， (1, n, 768)，辅助BERT区别句子对中的两个句子的向量表示
- Position Embeddings ，(1, n, 768) ，让BERT学习到输入的顺序属性
​ 这些表示会被按元素相加，得到一个大小为(1, n, 768)的合成表示。这一表示就是BERT编码层的输入了。


文章内容参考了
1. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
2. [Why BERT has 3 Embedding Layers and Their Implementation Details](https://mc.ai/why-bert-has-3-embedding-layers-and-their-implementation-details/)
3. [BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

接下去我会更新对BERT余下部分的学习和探究。


