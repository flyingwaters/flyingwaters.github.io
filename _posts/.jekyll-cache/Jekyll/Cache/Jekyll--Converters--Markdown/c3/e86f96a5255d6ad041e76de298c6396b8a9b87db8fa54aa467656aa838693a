I"6	<ul id="markdown-toc">
  <li><a href="#自然语言处理综论--第四章n元语法总结" id="markdown-toc-自然语言处理综论--第四章n元语法总结">自然语言处理综论  第四章n元语法（总结）</a>    <ul>
      <li><a href="#language-modelslm" id="markdown-toc-language-modelslm">Language Models，LM</a></li>
      <li><a href="#41-语料库中单词数目的计算" id="markdown-toc-41-语料库中单词数目的计算">4.1 语料库中单词数目的计算</a></li>
      <li><a href="#42-简单的非平滑的n元语法" id="markdown-toc-42-简单的非平滑的n元语法">4.2 简单的（非平滑的）N元语法</a></li>
      <li><a href="#43-平滑的n元语法" id="markdown-toc-43-平滑的n元语法">4.3 平滑的N元语法</a></li>
    </ul>
  </li>
</ul>

<h3 id="自然语言处理综论--第四章n元语法总结">自然语言处理综论  第四章n元语法（总结）</h3>
<p>通过(n-1)-gram 预测第N个单词的语言模型。
unigram 为通过一个单词预测下一个单词
bigram  为通过两个单词预测下一个单词
trigram  为通过三个单词预测下一个单词
这种统计概率的语言模型是基于语料库的。</p>

<h4 id="language-modelslm">Language Models，LM</h4>
<p>计算下一个单词的概率 与计算单词序列的概率密切相关。倾向于选择出现概率高的词或者句子。在句子无限长的情况下，可以得到较好预测整体准确率。</p>

<p>机器翻译中Veterbi算法后，通过N-gram判断句子通畅度， 手写识别和语音识别的N-gram纠错等。</p>

<p>预测下一个单词的能力对于残疾人的增强交际系统是至关重要的</p>

<h4 id="41-语料库中单词数目的计算">4.1 语料库中单词数目的计算</h4>

<p>概率的计算依赖于语料库(corpus), 计算单词的概率，基于Brown语料库规模为一百万单词的语料库</p>

<h4 id="42-简单的非平滑的n元语法">4.2 简单的（非平滑的）N元语法</h4>

<p>计算unigram 预测下一个词概率？通过语料库计算unigram 两个词联合分布。近似地逼近（approximate）该单词的历史。基于马尔可夫假设的unigram 预测下一个单词。
P(Wn | W(n-1,n-N-1)) ~ 逼近 N-gram 逼近全部单词序列预测下一个单词</p>

<h4 id="43-平滑的n元语法">4.3 平滑的N元语法</h4>

:ET