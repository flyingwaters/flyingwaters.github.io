I"»*<ul id="markdown-toc">
  <li><a href="#nlp-çš„æ–‡æœ¬ç›¸ä¼¼åº¦çŸ­è¯­å•è¯å¥å­" id="markdown-toc-nlp-çš„æ–‡æœ¬ç›¸ä¼¼åº¦çŸ­è¯­å•è¯å¥å­">NLP çš„æ–‡æœ¬ç›¸ä¼¼åº¦(çŸ­è¯­ã€å•è¯ã€å¥å­)</a>    <ul>
      <li><a href="#å‰è¨€" id="markdown-toc-å‰è¨€">å‰è¨€</a></li>
      <li><a href="#æ–‡ç« ç»“æ„" id="markdown-toc-æ–‡ç« ç»“æ„">æ–‡ç« ç»“æ„</a></li>
      <li><a href="#ç¬¬ä¸€éƒ¨åˆ†" id="markdown-toc-ç¬¬ä¸€éƒ¨åˆ†">ç¬¬ä¸€éƒ¨åˆ†</a>        <ul>
          <li><a href="#å’Œbertçš„ç›¸ä¼¼åº¦è®¡ç®—çš„å¯¹æ¯”-bertcosineçš„åº¦é‡" id="markdown-toc-å’Œbertçš„ç›¸ä¼¼åº¦è®¡ç®—çš„å¯¹æ¯”-bertcosineçš„åº¦é‡">å’ŒBERTçš„ç›¸ä¼¼åº¦è®¡ç®—çš„å¯¹æ¯”ï¼Œ BERT+COSINEçš„åº¦é‡</a></li>
          <li><a href="#tokenizer--stanford-corenlp-to-tokenizer" id="markdown-toc-tokenizer--stanford-corenlp-to-tokenizer">Tokenizer : stanford corenlp to tokenizer</a></li>
          <li><a href="#postagger-and-ner---postagger-and--ner-to-preprocessing-text-input-for-some-deep-learning-models" id="markdown-toc-postagger-and-ner---postagger-and--ner-to-preprocessing-text-input-for-some-deep-learning-models">Postagger and Ner :  postagger and  ner to preprocessing text input for some deep learning models</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#structural-features-ie-from-graph" id="markdown-toc-structural-features-ie-from-graph">Structural features (i.e from graph)</a>    <ul>
      <li><a href="#ä½¿ç”¨å›¾æå–ç‰¹å¾" id="markdown-toc-ä½¿ç”¨å›¾æå–ç‰¹å¾">ä½¿ç”¨å›¾æå–ç‰¹å¾</a></li>
      <li><a href="#å¦‚ä½•å»ºå›¾å’Œä½¿ç”¨å›¾æå–ç‰¹å¾" id="markdown-toc-å¦‚ä½•å»ºå›¾å’Œä½¿ç”¨å›¾æå–ç‰¹å¾">å¦‚ä½•å»ºå›¾å’Œä½¿ç”¨å›¾æå–ç‰¹å¾ï½</a></li>
    </ul>
  </li>
  <li><a href="#models" id="markdown-toc-models">Models</a></li>
  <li><a href="#bertæ–‡æœ¬ç›¸ä¼¼æ€§çš„å…¸å‹æ¡ˆä¾‹-åˆ©ç”¨é¢„è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹å®ç°åŸºäºbertçš„è¯­ä¹‰åŒ¹é…æ¨¡å‹-æ•°æ®é›†ä¸ºlcqmcæ•°æ®é›†" id="markdown-toc-bertæ–‡æœ¬ç›¸ä¼¼æ€§çš„å…¸å‹æ¡ˆä¾‹-åˆ©ç”¨é¢„è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹å®ç°åŸºäºbertçš„è¯­ä¹‰åŒ¹é…æ¨¡å‹-æ•°æ®é›†ä¸ºlcqmcæ•°æ®é›†">Bertæ–‡æœ¬ç›¸ä¼¼æ€§çš„å…¸å‹æ¡ˆä¾‹ åˆ©ç”¨é¢„è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹å®ç°åŸºäºBertçš„è¯­ä¹‰åŒ¹é…æ¨¡å‹ æ•°æ®é›†ä¸ºLCQMCæ•°æ®é›†</a></li>
  <li><a href="#lstmå¥å­ç›¸ä¼¼åº¦åˆ†æ" id="markdown-toc-lstmå¥å­ç›¸ä¼¼åº¦åˆ†æ">LSTMå¥å­ç›¸ä¼¼åº¦åˆ†æ</a></li>
  <li><a href="#query-çš„è§„åˆ™åŒ¹é…" id="markdown-toc-query-çš„è§„åˆ™åŒ¹é…">query çš„è§„åˆ™åŒ¹é…</a>    <ul>
      <li><a href="#é€Ÿåº¦å¿«-å¯æ§æ˜“äºå®ç°é«˜æ•ˆ" id="markdown-toc-é€Ÿåº¦å¿«-å¯æ§æ˜“äºå®ç°é«˜æ•ˆ">é€Ÿåº¦å¿«ï¼Œ å¯æ§ï¼Œæ˜“äºå®ç°ï¼Œé«˜æ•ˆã€‚</a></li>
    </ul>
  </li>
  <li><a href="#æ·±åº¦å­¦ä¹ è¯­ä¹‰åŒ¹é…" id="markdown-toc-æ·±åº¦å­¦ä¹ è¯­ä¹‰åŒ¹é…">æ·±åº¦å­¦ä¹ è¯­ä¹‰åŒ¹é…</a></li>
</ul>

<h1 id="nlp-çš„æ–‡æœ¬ç›¸ä¼¼åº¦çŸ­è¯­å•è¯å¥å­">NLP çš„æ–‡æœ¬ç›¸ä¼¼åº¦(çŸ­è¯­ã€å•è¯ã€å¥å­)</h1>
<h2 id="å‰è¨€">å‰è¨€</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œä¼šç»å¸¸æ¶‰åŠå‡ ç§è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬ç›¸ä¼¼åº¦çš„é—®é¢˜ã€‚æå‡ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿçš„å¬å›ç‡ï¼Œé—®ç­”ç³»ç»Ÿä¸­çš„ï¼Œéœ€è¦è®¡ç®—çš„queryå’Œå€™é€‰é›†çš„ç›¸ä¼¼åº¦;  ## æ–‡æœ¬ç›¸ä¼¼åº¦æ–¹æ³• * åŸºäºç»Ÿè®¡çš„æ–¹æ³•     ä¸»è¦æ˜¯æ— ç›‘ç£å­¦ä¹ ï¼Œå¥å­ã€æ®µè½, è¾ƒå¤§ç²’åº¦çš„æ–‡æœ¬ã€‚ * åŸºäºè¯­ä¹‰çš„æ–¹æ³•     å¤§éƒ¨åˆ†æ˜¯åŸºäºæ·±åº¦å­¦ä¹ çš„æœ‰ç›‘ç£å­¦ä¹ ï¼Œä¸€èˆ¬è¯è¯­æˆ–å¥å­, è¾ƒå°ç²’åº¦çš„æ–‡æœ¬ã€‚
</code></pre></div></div>

<h2 id="æ–‡ç« ç»“æ„">æ–‡ç« ç»“æ„</h2>
<ol>
  <li>å¸¸è§çš„å‡ ç§ç›¸ä¼¼æ€§æ¯”è¾ƒæ–¹æ³•</li>
</ol>

<ul>
  <li>
    <p>åŸºäºè¯å‘é‡</p>
  </li>
  <li>
    <p>åŸºäºå­—ç¬¦</p>
  </li>
  <li>
    <p>åŸºäºæ¦‚ç‡ç»Ÿè®¡</p>
  </li>
  <li>
    <p>åŸºäºè¯åµŒå…¥æ¨¡å‹</p>
  </li>
</ul>

<ol>
  <li>State of the art çš„å‡ ç§ä»£è¡¨æ€§æ–¹æ³•</li>
</ol>

<h2 id="ç¬¬ä¸€éƒ¨åˆ†">ç¬¬ä¸€éƒ¨åˆ†</h2>
<ol>
  <li>
    <p>è±æ–‡æ–¯å¦è·ç¦»(ç¼–è¾‘è·ç¦»)
ä¸€ä¸ªå­—ç¬¦ä¸²æ”¹æˆå¦ä¸€ä¸ªå­—ç¬¦ä¸²çš„æœ€å°‘ç¼–è¾‘æ“ä½œï¼Œâ€œåº¦å‡è‡ªç”±è¡Œâ€å’Œâ€œè‡ªç”±è¡Œåº¦å‡â€ï¼Œä¸­æ–‡æ„æ€å®Œå…¨ä¸€æ ·ï¼Œä½†æ˜¯ç¼–è¾‘è·ç¦»æ˜¯5ï¼Œ è¡¨ç¤ºç›¸ä¼¼åº¦å¾ˆä½ã€‚</p>
  </li>
  <li>
    <p>æ¬§å¼å’Œä½™å¼¦è·ç¦»
åŸºäºå‘é‡ç©ºé—´çš„è·ç¦»åº¦é‡æ–¹æ³•</p>
  </li>
</ol>

<h3 id="å’Œbertçš„ç›¸ä¼¼åº¦è®¡ç®—çš„å¯¹æ¯”-bertcosineçš„åº¦é‡">å’ŒBERTçš„ç›¸ä¼¼åº¦è®¡ç®—çš„å¯¹æ¯”ï¼Œ BERT+COSINEçš„åº¦é‡</h3>
<ol>
  <li>åˆ©ç”¨Bertè®¡ç®—å•è¯æˆ–è€…çŸ­è¯­ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚</li>
</ol>

<p>ä¸­å›½è¶³åå’Œä¸­å›½è¶³çƒçš„å‘é‡ç›¸ä¼¼åº¦: 0.94828343
ä¸­å›½è¶³åå’Œä¸­å›½ç¯®åçš„å‘é‡ç›¸ä¼¼åº¦: 0.9531492
è‡ªç”±è¡Œåº¦å‡å’Œåº¦å‡è‡ªç”±è¡Œç›¸ä¼¼åº¦: 0.97110826
é—®é¢˜ï¼š</p>

<p>æ˜æ˜¾çœ‹å‡ºè‡ªç”±è¡Œåº¦å‡å’Œåº¦å‡è‡ªç”±è¡Œçš„ç›¸ä¼¼åº¦ï¼Œæ˜¯é«˜äºå‰ä¸¤ä¸ªçš„ï¼Œè¯æ˜BERTä½“ç³»å¯ä»¥å¾ˆå¥½èµ·åˆ°ç¼–è¾‘è·ç¦»çš„ä½œç”¨ã€‚</p>

<ol>
  <li>N-gram è·ç¦»ï¼ˆBLEUæ ¸å¿ƒæ€æƒ³ï¼‰
Similarity  = |Gn(S)|+|Gt(T)|-2<em>|Gn(S)|</em>|Gn(T)|
å­—ç¬¦ä¸²å®Œå…¨ç›¸ç­‰ == 0</li>
</ol>

<ul>
  <li>ä¸­å›½è¶³åå’Œä¸­å›½è¶³çƒçš„å‘é‡ç›¸ä¼¼åº¦: 0.94828343</li>
  <li>ä¸­å›½è¶³åå’Œä¸­å›½ç¯®åçš„å‘é‡ç›¸ä¼¼åº¦: 0.9531492</li>
  <li>ä¸­å›½è¶³åå’Œè¶³åä¸­å›½çš„å‘é‡ç›¸ä¼¼åº¦ 0.9493892</li>
  <li>ç¾ä¸½ä¸­å›½å’Œä¸­å›½ç¾ä¸½çš„å‘é‡ç›¸ä¼¼åº¦ 0.95919204</li>
  <li>è‡ªç”±è¡Œåº¦å‡å’Œåº¦å‡è‡ªç”±è¡Œç›¸ä¼¼åº¦: 0.97110826
ç›¸å¯¹äºN-gramçš„è·ç¦»ï¼ŒåŠ å…¥æ›´å¤šè¯­ä¹‰ä¿¡æ¯ä½œä¸ºè¯çš„è¡¨ç¤º</li>
</ul>

<ol>
  <li>
    <p>BM25 
OKapi BM25 (BMä»£è¡¨æœ€ä½³åŒ¹é…ï¼‰æ˜¯æœç´¢å¼•æ“æ ¹æ®å…¶ä¸ç»™å®šçš„æœç´¢æŸ¥è¯¢çš„ç›¸å…³æ€§å¯¹åŒ¹é…æ–‡æ¡£è¿›è¡Œæ’åçš„æ’åå‡½æ•°ã€‚Stenphen E. Robertson å’ŒKarenSparckJones ç­‰äººå¼€å‘çš„æ¦‚ç‡æ£€ç´¢æ¡†æ¶ã€‚
query -&gt; è‹¥å¹²ç»“æœDï¼Œ</p>
  </li>
  <li>
    <p>WMD
EMD åœ¨NLPé¢†åŸŸçš„å»¶ä¼¸ã€‚EMDè·ç¦»æ˜¯ä¸€ç±»è¿è¾“é—®é¢˜ã€‚ 
WMDæ˜¯2015å¹´æå‡ºçš„ä¸€ç§è¡¡é‡æ–‡æœ¬ç›¸ä¼¼åº¦çš„æ–¹æ³•ï¼Œworld  Moverâ€™s Distance@ (ç®€ç§°)WMD ,è¯ç§»è·ç¦»ï¼Œåº¦é‡ä¸¤ä¸ªæ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œå³WMDè·ç¦»è¶Šå°ç›¸ä¼¼åº¦è¶Šå¤§ã€‚</p>
  </li>
</ol>

<p>DOC2VECï¼Œ simHashåŸç†ä»‹ç»</p>

<p># the some ideas</p>
<ol>
  <li>Features 
    three kind of featuresï¼š
        1.  word embedding 
        2. Sentence embeddings (Doc2Vec,  Sent2Vec)
        3.  Encode question pair using dense layer from ESIM model trained on SNLI</li>
  <li>Classical text mining features
    <ul>
      <li>Similarity meatures  on LDA  and LSI embedding</li>
      <li>Similarity meatures on bag of character n-grams (TFIDF reweighted or not )  from 1 to 8 gram</li>
      <li>Abhishekâ€™s and owlâ€™s kindly shared features</li>
      <li>
        <p>Edit and sequence matching distance ,  percentage of  common tokens up to 1,2,â€¦..,6 when question ends the same,  or starts the same</p>
      </li>
      <li>Length of  questions, diff of  legnth</li>
      <li>Number of capital letters,  question marks etcâ€¦</li>
      <li>Indicators  for  Question 1/2 starting  with  â€œareâ€,â€ Canâ€, â€œHowâ€ etc â€¦. and all mathematical engineering corresponding</li>
    </ul>
  </li>
</ol>

<h3 id="tokenizer--stanford-corenlp-to-tokenizer">Tokenizer : stanford corenlp to tokenizer</h3>
<h3 id="postagger-and-ner---postagger-and--ner-to-preprocessing-text-input-for-some-deep-learning-models">Postagger and Ner :  postagger and  ner to preprocessing text input for some deep learning models</h3>

<h1 id="structural-features-ie-from-graph">Structural features (i.e from graph)</h1>
<h4 id="ä½¿ç”¨å›¾æå–ç‰¹å¾">ä½¿ç”¨å›¾æå–ç‰¹å¾</h4>
<h4 id="å¦‚ä½•å»ºå›¾å’Œä½¿ç”¨å›¾æå–ç‰¹å¾">å¦‚ä½•å»ºå›¾å’Œä½¿ç”¨å›¾æå–ç‰¹å¾ï½</h4>
<ol>
  <li>
    <p>We  built density features from the graph built  from the edges  between pairs of questions inside  train and  test datasets  concatenated.</p>
  </li>
  <li>
    <p>We  had  counts  of   neighbors  of  question  1,  question  2,  the min,  the max,  intersections, unions,  shortest  path  length  when main edge cutâ€¦..</p>
  </li>
  <li>
    <p>We went further  and built density features to count the neighbors of  the  questions neighborsâ€¦.. and  questions neighbors neighbors â€¦ (inception)</p>
  </li>
  <li>
    <p>We also counted neighbors  of higher order which also were  neighbors  of  lower  order (loops)</p>
  </li>
  <li>
    <p>We tried  different  graph  structures  :  we built undirected and directed  graphs  (edges  directed from question 1 to question 2 ),  we  also tried to separate the  density features  of question 1 from the features  of  question 2 to generate non commutativeï¼ˆäº¤æ¢çš„ï¼‰features  in addition to commutative ones</p>
  </li>
  <li>
    <p>We built features describing the connex((è¿é€š) subgraph the pair belonged to :  Number of edges ,  number of nodes, % of  edge in train</p>
  </li>
  <li>
    <p>We also computed the same features on subgrphs built only from the edges of questions which both appear more than once.æˆ‘ä»¬æƒ³åšçš„æ˜¯ç§»é™¤fake questions which we thought were damaging the graph features by changing its  structure.</p>
  </li>
  <li>
    <p>æœ€åï¼Œæˆ‘ä»¬ç»™graph å¢åŠ äº†æƒé‡ï¼Œ æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ç§æˆ‘ä»¬ç›¸ä¼¼åº¦çš„ç‰¹å¾ç»™æˆ‘ä»¬çš„graphå¢åŠ æƒé‡ã€‚</p>
  </li>
</ol>

<h1 id="models">Models</h1>

<ol>
  <li>
    <p>We worked on two main  architectures for our NNets: Siamese and Attention Neutral Networks.</p>
  </li>
  <li>
    <p>Siamese LSTM with pretrained GLOVE embedding</p>
  </li>
  <li>
    <p>Decomposable attention with pretrained FastText embedding . This model achive ~0.3 on cv</p>
  </li>
  <li>
    <p>ESIM with pretrained FastText embedding .  this is our best pure Deep Learning NLP model ,  it achieves ~ 0.27 on CV.  However this model take too long to run , we only add it once in the first stacking layer</p>
  </li>
  <li>
    <p>We noticed that DL complex  works on the first stacking layer but did not do better than simple  MLP  on second  layer</p>
  </li>
</ol>

<h1 id="bertæ–‡æœ¬ç›¸ä¼¼æ€§çš„å…¸å‹æ¡ˆä¾‹-åˆ©ç”¨é¢„è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹å®ç°åŸºäºbertçš„è¯­ä¹‰åŒ¹é…æ¨¡å‹-æ•°æ®é›†ä¸ºlcqmcæ•°æ®é›†">Bertæ–‡æœ¬ç›¸ä¼¼æ€§çš„å…¸å‹æ¡ˆä¾‹ åˆ©ç”¨é¢„è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹å®ç°åŸºäºBertçš„è¯­ä¹‰åŒ¹é…æ¨¡å‹ æ•°æ®é›†ä¸ºLCQMCæ•°æ®é›†</h1>
<p>https://github.com/pengming617/bert_textMatching</p>

<h1 id="lstmå¥å­ç›¸ä¼¼åº¦åˆ†æ">LSTMå¥å­ç›¸ä¼¼åº¦åˆ†æ</h1>
<p>https://github.com/zqhZY/semanaly</p>

<h1 id="query-çš„è§„åˆ™åŒ¹é…">query çš„è§„åˆ™åŒ¹é…</h1>
<h4 id="é€Ÿåº¦å¿«-å¯æ§æ˜“äºå®ç°é«˜æ•ˆ">é€Ÿåº¦å¿«ï¼Œ å¯æ§ï¼Œæ˜“äºå®ç°ï¼Œé«˜æ•ˆã€‚</h4>
<ol>
  <li>ä¸FAQåº“ä¸­çš„æ ‡é—®å’Œç›¸ä¼¼æ–‡é—®ï¼Œ è¿›è¡Œåˆ†è¯ã€æç‚¼å‡ºå¤§é‡çš„æ¦‚å¿µï¼Œå¹¶å°†ä¸Šè¿°æ¦‚å¿µç»„åˆï¼Œæ„æˆå¤§é‡çš„å¥å¼ï¼Œ
 åä¸ºmate30 æ˜¯cellphoneæ¦‚å¿µï¼Œaskmoneyæ¦‚å¿µ-ï¼Œ â€œç°åœ¨â€ æ˜¯æ—¶é—´æ¦‚å¿µã€‚</li>
</ol>

<h1 id="æ·±åº¦å­¦ä¹ è¯­ä¹‰åŒ¹é…">æ·±åº¦å­¦ä¹ è¯­ä¹‰åŒ¹é…</h1>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DSSMï½ LSTM-DSSM
åŸºäºBERTç­‰é¢„è®­ç»ƒæ¨¡å‹   #   Knowledge Based Question &amp;Answer   åŸºäºçŸ¥è¯†ä½“ç³»çš„é—®ç­”ç³»ç»Ÿ
</code></pre></div></div>

<p>KBQA æœ€å…³é”®ä¸€æ­¥åœ¨äºKGçš„æ­å»ºï¼Œå¯¹ç»å¤§éƒ¨åˆ†NLPä»»åŠ¡éƒ½æœ‰æå¤§æå¤§çš„åŠ æˆã€‚</p>

:ET