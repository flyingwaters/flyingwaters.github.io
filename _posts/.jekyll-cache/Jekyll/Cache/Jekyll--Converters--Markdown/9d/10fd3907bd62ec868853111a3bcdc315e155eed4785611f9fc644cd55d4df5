I"<ul id="markdown-toc">
  <li><a href="#2019过去了人生如白驹过隙真的很快" id="markdown-toc-2019过去了人生如白驹过隙真的很快">2019过去了，人生如白驹过隙，真的很快</a>    <ul>
      <li><a href="#题外话" id="markdown-toc-题外话">题外话</a></li>
    </ul>
  </li>
  <li><a href="#bert的细节" id="markdown-toc-bert的细节">BERT的细节</a></li>
  <li><a href="#token-embedding" id="markdown-toc-token-embedding">token embedding</a></li>
  <li><a href="#segment-embedding" id="markdown-toc-segment-embedding">segment embedding</a></li>
  <li><a href="#position-embedding" id="markdown-toc-position-embedding">Position Embedding</a>    <ul>
      <li><a href="#合成表示" id="markdown-toc-合成表示">合成表示</a></li>
    </ul>
  </li>
</ul>

<h3 id="2019过去了人生如白驹过隙真的很快">2019过去了，人生如白驹过隙，真的很快</h3>

<p>人工智能在慢慢一点点发展，BERT，基于两个思想，word2vec &lt;—- 条件概率 &lt;—- n-gram语言模型， 看了word2vec精彩的数学推论，然后看了看bert，突然有个想法，AI是一群孜孜不倦的人共同发展出来的，它还需要更多的人去拓展。</p>
<h4 id="题外话">题外话</h4>
<ol>
  <li>我感觉nlp发展有时真的靠直觉，人类最淳朴的直觉。当然最基本的还是有基础知识。。。。。</li>
  <li>BERT模型，由seq2seq 或者说encoder-decoder 这没记错是hinton老爷子，搞出来的，被称为神经网络的表示学习，表示学习，也就是说信息冗余，每个数据都是冗余，我们可以用很少的知识恢复它，这不是提取本质，抛开虚伪直达本真~~~也许不准确，换种说法更加高级而且灵活的，自适应SVD.。。。。</li>
  <li>BERT乃是nlp面试必考，请大家务必搞熟，另外作为nlp预训练模型的基石，我想掌握BERT理论架构，并且熟练使用它，显然是必须的，这是一系列的笔记。关于BERT理论和应用以及代码使用，除此之外还可能会涉及一点Transformer和Attention的历史以及现状。</li>
</ol>

<h3 id="bert的细节">BERT的细节</h3>
<p>如何看懂这是一个很稳的问题：
首先对于整体结构，我们可以百度、google然后打开blog有很多说明。我关注的非常实际的两个方面：1、理论层面理解bert的好处。2、官方开源代码的使用（超参的设置）
首先我们从理论角度出发，详细分析bert模型的输入部分：
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/09/bert_emnedding.png" alt="alt 文字属性" />
                        <center>**图 1**</center>
bert模型的输入需要三个嵌入层</p>

<ol>
  <li>Token Embedding层</li>
  <li>Segment Embedding层</li>
  <li>Position Embedding层
接下去我要细致的分析它们的运行过程和作用细节:</li>
</ol>

<h2 id="token-embedding">token embedding</h2>
<p>文本每个词 —&gt; token embedding层，其具体过程如下每个词或者字—–&gt;(768,)表示。这里768是bert一个基础版的参数，可以称为bert-base-uncased、其他的如bert-large-uncased等为1024。每个文本单位（中文为字、英文为词）的向量表示维度是不同的。
例子如下:
<img src="https://thumbnail0.baidupcs.com/thumbnail/72537c9ecn8566c40f27a46f983e338c?fid=221327382-250528-455884105595335&amp;time=1578934800&amp;rt=sh&amp;sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-ckY7XJTUyMTQYLpnlWNUj0q%2B%2B6Y%3D&amp;expires=8h&amp;chkv=0&amp;chkbd=0&amp;chkpc=&amp;dp-logid=302995822538476589&amp;dp-callid=0&amp;size=c710_u400&amp;quality=100&amp;vuk=-&amp;ft=video" alt="alt 文字属性" />
            <center>**图 2**</center>
如5个汉字—&gt; (5, 768)
值得注意的是英文中我们采用wordpiece，这种方法会将英文拆分为piece，singing—&gt;sing  ##ing,具体细则，我们可以看<a href="https://arxiv.org/pdf/1609.08144.pdf">Wu et al. (2016)</a>这里有详细的讲解。</p>

<h2 id="segment-embedding">segment embedding</h2>
<p>这个简单的说就是区分，输入是一个句子还是两个，如果只有一个句子，那么全是0，如果有两个句子，前一个1， 后一个为0.如图1所示。</p>

<h2 id="position-embedding">Position Embedding</h2>
<p>transformer中的position encode公式如下：
<script type="math/tex">PE(pos,2i)=sin(pos/10000^{2i/dmodel})</script>
<script type="math/tex">PE(pos,2i+1)=cos(pos/10000^{2i/dmodel})</script>
其中，pos是指词语在序列中的位置。可以看出，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码。上面公式中的dmodel是模型的input维度(不够时需要padding)，论文默认是512。</p>

<p>这个编码公式的意思就是：给定词语的位置pos，我们可以把它编码成dmodel维的向量！也就是说，位置编码的每一个维度对应正弦曲线，波长构成了从2π到10000∗2π的等比序列。上面的位置编码是绝对位置编码。但是词语的相对位置也非常重要。这就是论文为什么要使用三角函数的原因！正弦函数能够表达相对位置信息。</p>

<p>主要数学依据是以下两个公式：
<script type="math/tex">sin(α+β)=sinαcosβ+cosαsinβsin(α+β)=sinαcosβ+cosαsinβ</script>
<script type="math/tex">cos(α+β)=cosαcosβ−sinαsinβcos(α+β)=cosαcosβ−sinαsinβ</script>
上面的公式说明，对于词汇之间的位置偏移k，$PE(pos+k)—&gt;sin(\beta+\alpha)$可以表示成$PE(pos)—&gt;sin(\alpha)$和$PE(k)—&gt;sin(\beta)$的组合形式，这就是表达相对位置的能力！</p>

<p>但是Bert中是训练出来的(512, 768 or N)的lookup—&gt; 也就是学习了位置表示。</p>
<h3 id="合成表示">合成表示</h3>
<p>我们已经介绍了长度为n的输入序列将获得的三种不同的向量表示，分别是：</p>

<ul>
  <li>Token Embeddings， (1, n, 768) ，词的向量表示</li>
  <li>Segment Embeddings， (1, n, 768)，辅助BERT区别句子对中的两个句子的向量表示</li>
  <li>Position Embeddings ，(1, n, 768) ，让BERT学习到输入的顺序属性
​ 这些表示会被按元素相加，得到一个大小为(1, n, 768)的合成表示。这一表示就是BERT编码层的输入了。</li>
</ul>

<p>文章内容参考了</p>
<ol>
  <li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
  <li><a href="https://mc.ai/why-bert-has-3-embedding-layers-and-their-implementation-details/">Why BERT has 3 Embedding Layers and Their Implementation Details</a></li>
  <li><a href="https://arxiv.org/abs/1810.04805">BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
</ol>

<p>接下去我会更新对BERT余下部分的学习和探究。</p>

:ET