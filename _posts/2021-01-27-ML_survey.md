---
layout: post
title: "数据人工智能的概念"
categories: NLP
tags:  embedding
author: fly
---

* content
{:toc}

######  终身学习、迁移学习、元学习、机器学习
从全球人工智能技术创新大赛
热身赛二: 中文预训练模型泛化能力挑战赛
引出，data science的中的几种不同相关学习范式。

概念区分:
 元学习 和 迁移学习 并无本质区分都是增加学习器在多任务的泛化能力。但元学习更偏重任务和数据双重采样，任务和数据一样需要采样，学习到F(X)帮助在未见过的f(x)里迅速建立mapping。而迁移学习更多指从一个任务迁移到其他任务的能力迁移，不太强调任务空间的概念。

机器学习：一个具体任务的函数拟合

元学习: 元学习的概念框架。元学习从大量任务训练模型，并通过少量数据在新任务中更快地学习。

训练元学习，快速元学习器，少量预测样本的类别，不仅可以使快速学习器学习每一个子集的分类，还可以使它抽取类别之间的共和特性

迁移学习：相同任务下，源域学习，目标域迁移

多任务学习：MTL loss, joint learning, auxiliary task 先验任务来提高模型的泛化性。和L1 loss具有sparse model的先验具有相同想过，倾向于约束模型优化方向为joint loss。

？问题一：深度学习的多个loss如何平衡？
在多任务很多的监督学习领域，如何平和多任务loss的weight或者说，更好的分布.
Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics
CVPR 2018 的paper Alex Kendall Yarin Gal
这种多任务loss的调整方法，应具有普适性，适应多个领域。








